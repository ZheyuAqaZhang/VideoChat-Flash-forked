datasets:
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/image/textcaps.json
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/LLaVA-OneVision-Data/textcapsextracted_images/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/image/textocr(gpt4v).json
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/LLaVA-OneVision-Data/textocr(gpt4v)extracted_images/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/image/rendered_text(cauldron)_fix.json
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/LLaVA-OneVision-Data/rendered_text(cauldron)extracted_images/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/image/iam(cauldron)_fix.json
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/LLaVA-OneVision-Data/iam(cauldron)extracted_images/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/image/llavar_gpt4_20k.json
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/LLaVA-OneVision-Data/llavar_gpt4_20kextracted_images/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/image/allava_instruct_vflan4v.json
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/LLaVA-OneVision-Data/allava_instruct_vflan4vextracted_images/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/image/allava_instruct_laion4v.json
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/LLaVA-OneVision-Data/allava_instruct_laion4vextracted_images/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/image/sharegpt4o.json
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/LLaVA-OneVision-Data/sharegpt4oextracted_images/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/image/sharegpt4v(coco).json
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/LLaVA-OneVision-Data/sharegpt4v(coco)extracted_images/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/image/sharegpt4v(knowledge).json
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/LLaVA-OneVision-Data/sharegpt4v(knowledge)extracted_images/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/image/sharegpt4v(llava).json
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/LLaVA-OneVision-Data/sharegpt4v(llava)extracted_images/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/image/sharegpt4v(sam).json
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/LLaVA-OneVision-Data/sharegpt4v(sam)extracted_images/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/image/tallyqa(cauldron,llava_format)_fix.json
  sampling_strategy: first:0.4%
  data_root: OpenGVLab/stage3/LLaVA-OneVision-Data/tallyqa(cauldron,llava_format)extracted_images/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/image/st_vqa(cauldron,llava_format)_fix.json
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/LLaVA-OneVision-Data/st_vqa(cauldron,llava_format)extracted_images/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/image/llava_next_raw_format_processed_738k.json
  sampling_strategy: first:0.5%
  data_root: OpenGVLab/stage3/LLaVA-NeXT-Data/llava_next_raw_format/
- json_path: OpenGVLab/stage3/M4-Instruct-Data/m4_instruct_annotations.json
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/M4-Instruct-Data/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/caption_sharegemini_k400_223k.json
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage2/OpenMMLab___Kinetics-400/raw/Kinetics-400/videos_train/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/caption_textvr-textvr-train_40k.json
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/textvr/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/moviechat1k_caption-MovieChat-train_caption_1k.json
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/MovieChat-1K_train/raw_videos/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/caption_favd-favd-train_10k.json
  sampling_strategy: first:0.5%
  data_root: OpenGVLab/stage3/favd/videos/train/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/caption_sharegptvideo_300k-sharegptvideo-train_300k_302k.json
  sampling_strategy: first:0.5%
  data_root: OpenGVLab/train_video_and_instruction/train_300k/
  video_read_type: img
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/caption_sharegpt4o-sharegpt4o_3k.json
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/ShareGPT-4o/pvideo/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/vqa_tvqa-tvqa_123k.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/tvqa_data/frames_hq/merged/
  video_read_type: img
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/vqa_tgif_transition_qa-tgif_transition_qa-train_53k.jsonl
  sampling_strategy: first:0.5%
  data_root: OpenGVLab/stage3/tgif/gifs/
  video_read_type: gif
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/reasoning_clevrer_mc-clevrer_mc-train_43k_debug_43k.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/clevrer/merged/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/reasoning_clevrer_qa-clevrer_qa-train_mc_40k.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/clevrer/merged/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/classification_ssv2-ssv2-train_40k.jsonl
  sampling_strategy: first:0.5%
  data_root: OpenGVLab/stage3/ssv2/20bn-something-something-v2/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/vqa_perception_train-mc_question_train_forchoice_8k.json
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/perception/videos/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/vqa_tgif_transition_qa_openend-openend_qa_annos-tgif_transition_qa_train_openend_53k.jsonl
  sampling_strategy: first:0.5%
  data_root: OpenGVLab/stage3/tgif/gifs/
  video_read_type: gif
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/vqa_tgif_frame_qa-tgif_frame_qa-train_40k.jsonl
  sampling_strategy: first:0.5%
  data_root: OpenGVLab/stage3/tgif/gifs/
  video_read_type: gif
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/vqa_tgif_count-openend_qa_train_openend_26839.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/tgif/gifs/
  video_read_type: gif
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/vqa_tgif_action-openend_qa_train_openend_20471.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/tgif/gifs/
  video_read_type: gif
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/reasoning_next_qa_oe-openend_qa_annos-next_qa_train_openend_35k.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/nextqa/NExTVideo/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/moviechat1k_global-MovieChat-train_global_1k.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/MovieChat-1K_train/raw_videos/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/vqa_sharegptvideo_240k-sharegptvideo-train_240k_240k.json
  sampling_strategy: first:0.5%
  data_root: OpenGVLab/train_video_and_instruction/train_300k/
  video_read_type: img
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/caption_vidln_oops-vidln-oops_train_11k.json
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/oops/oops_dataset/oops_video/train/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/caption_vidln_ovis-vidln-ovis_train_1k.json
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/ovis/Images/train/train/c1a40349/
  video_read_type: img
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/caption_vidln_uvo_sparse-vidln-uvo_sparse_train_6k.json
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/uvo/uvo_videos_sparse/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/caption_vidln_uvo_dense-vidln-uvo_dense_train_1k.json
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/uvo/uvo_videos_dense/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/vript_long_videos_en_20240911_fix.jsonl
  sampling_strategy: first:0.5%
  data_root: OpenGVLab/stage3/Vript/vript_long_videos_clips/videos/merged/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/vript_short_videos_en_20240911_fix.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/Vript/vript_short_videos_clips/videos/merged/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/guiworld_en_20241029_fix.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/stage3/GUI-World/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_2_3_m_academic_mc_v0_1_qa_processed_6901_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_2_3_m_nextqa_oe_qa_processed_61_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_0_30_s_youtube_oe_v0_1_qa_processed_420200_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_1_2_m_academic_oe_v0_1_qa_processed_26302_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_1_2_m_youtube_mc_v0_1_qa_processed_39710_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_30_60_s_nextqa_oe_qa_processed_6843_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_2_3_m_youtube_mc_v0_1_qa_processed_39967_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_2_3_m_academic_v0_1_cap_processed_3124_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_30_60_s_academic_oe_v0_1_qa_processed_57924_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_2_3_m_youtube_v0_1_cap_processed_24685_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_30_60_s_youtube_mc_v0_1_qa_processed_39927_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_0_30_s_activitynetqa_oe_qa_processed_2950_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_1_2_m_nextqa_oe_qa_processed_4694_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_30_60_s_youtube_oe_v0_1_qa_processed_110624_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_1_2_m_academic_mc_v0_1_qa_processed_4241_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_0_30_s_youtube_mc_v0_1_qa_processed_39353_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_30_60_s_activitynetqa_oe_qa_processed_4530_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_1_2_m_youtube_oe_v0_1_qa_processed_137645_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_30_60_s_academic_mc_v0_1_qa_processed_20346_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_30_60_s_youtube_v0_1_cap_processed_19995_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_0_30_s_nextqa_mc_qa_processed_5496_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_0_30_s_academic_mc_v0_1_qa_processed_5753_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_2_3_m_youtube_oe_v0_1_qa_processed_141495_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_1_2_m_nextqa_mc_qa_processed_4633_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_2_3_m_activitynetqa_oe_qa_processed_7460_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_2_3_m_nextqa_mc_qa_processed_52_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_1_2_m_activitynetqa_oe_qa_processed_8590_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_1_2_m_academic_v0_1_cap_processed_4627_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_30_60_s_academic_v0_1_cap_processed_10514_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_1_2_m_youtube_v0_1_cap_processed_24234_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_30_60_s_nextqa_mc_qa_processed_6843_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_0_30_s_nextqa_oe_qa_processed_5492_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_0_30_s_academic_oe_v0_1_qa_processed_48468_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_0_30_s_youtube_v0_1_cap_processed_79346_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_2_3_m_academic_oe_v0_1_qa_processed_18134_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_0_30_s_perceptiontest_mc_qa_processed_1785_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_30_60_s_perceptiontest_mc_qa_processed_618_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/llava-video_0_30_s_academic_v0_1_cap_processed_11985_with_duration.jsonl
  sampling_strategy: first:2%
  data_root: OpenGVLab/LLaVA-Video-178K/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/htstep_eventunderstanding-longvideo_annos-htstep_eventunderstanding_1k_1k.json
  sampling_strategy: first:2%
  video_read_type: img
  data_root: OpenGVLab/VideoChat-Flash-Training-Data/longvid_subset/htstep_eventunderstanding_1k/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/htstep_eventcount-longvideo_annos-htstep_eventcount_2k_2k.json
  sampling_strategy: first:2%
  video_read_type: img
  data_root: OpenGVLab/VideoChat-Flash-Training-Data/longvid_subset/htstep_eventcount_2k/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/htstep_eventrelationship-longvideo_annos-htstep_eventrelationship_1k_1k.json
  sampling_strategy: first:2%
  video_read_type: img
  data_root: OpenGVLab/VideoChat-Flash-Training-Data/longvid_subset/htstep_eventrelationship_1k/
- json_path: OpenGVLab/VideoChat-Flash-Training-Data/annotations/video/ego4dhcap_eventunderstanding-longvideo_annos-ego4dhcap_eventunderstanding_2k_2k.json
  sampling_strategy: first:2%
  video_read_type: img
  data_root: OpenGVLab/VideoChat-Flash-Training-Data/longvid_subset/ego4dhcap_eventunderstanding_2k/
